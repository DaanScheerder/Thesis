{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download the data and prepare the data files for later steps."
      ],
      "metadata": {
        "id": "iOSgqGwJYV_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5kdLa5mOs5z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsB4GC5Npn69"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emD8IqSpp01T"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/THESIS/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m9gWXidqFoR"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/THESIS/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EZTO6loqLsz"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PysQjeNUpZ_9"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/THESIS/ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\" -d \"/content/drive/MyDrive/THESIS/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuqP1b4FzgkO"
      },
      "outputs": [],
      "source": [
        "# move all files to main folder\n",
        "source_folder = r\"/content/drive/MyDrive/THESIS/data/UkraineWar/UkraineWar/\"\n",
        "destination_folder = r\"/content/drive/MyDrive/THESIS/data/\"\n",
        "\n",
        "for file_name in os.listdir(source_folder):\n",
        "    source = source_folder + file_name\n",
        "    destination = destination_folder + file_name\n",
        "    if os.path.isfile(source):\n",
        "        shutil.move(source, destination)\n",
        "        print('Moved:', file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWThCTHd0z0P"
      },
      "outputs": [],
      "source": [
        "# remove empty folders\n",
        "os.rmdir(\"/content/drive/MyDrive/THESIS/data/UkraineWar/UkraineWar\")\n",
        "os.rmdir(\"/content/drive/MyDrive/THESIS/data/UkraineWar/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort files by date for easier selection later\n",
        "files_dates= pd.DataFrame()\n",
        "\n",
        "for n in os.listdir('/content/drive/MyDrive/THESIS/data'):\n",
        "  if n[0:4].isdigit():\n",
        "    if n[0:4]=='2023':\n",
        "      date = n[0:4]+\"-\"+n[4:6]+\"-\"+n[6:8]\n",
        "    else:\n",
        "      date = \"2022-\"+n[0:2]+\"-\"+n[2:4]\n",
        "  else:\n",
        "    if n[29:34].isdigit() == False:\n",
        "      if n[29:32] == \"FEB\":\n",
        "        date = \"2022-02-\"+n[32:34]\n",
        "      if n[29:32] == \"MAR\":\n",
        "        date = \"2022-03-\"+n[32:34]\n",
        "    else:\n",
        "      date = n[28:32]+\"-\"+n[32:34]+\"-\"+n[34:36]\n",
        "  f = '/content/drive/MyDrive/THESIS/data/'+ n\n",
        "  files_dates = files_dates.append(pd.Series([f, date]), ignore_index=True)\n",
        "\n",
        "files_dates.columns = ['files', 'date']\n",
        "files_dates['date'] = pd.to_datetime(files_dates['date'])\n",
        "files_dates = files_dates.sort_values(by='date', ascending=True)\n",
        "files_dates = files_dates.reset_index(drop=True)\n",
        "#files_dates = files_dates.set_index(['date'])\n",
        "files_dates.to_pickle(\"/content/drive/MyDrive/THESIS/files_dates.pkl\")"
      ],
      "metadata": {
        "id": "RrSJbFvrL4KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "files_dates = pd.read_pickle(\"/content/drive/MyDrive/THESIS/files_dates.pkl\")"
      ],
      "metadata": {
        "id": "D3zI5HQw38ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine files with same date\n",
        "filenames = list(files_dates[0:2]['files'])\n",
        "combined_csv = pd.concat([pd.read_csv(f, compression='gzip', low_memory=False)for f in filenames ])\n",
        "combined_csv.to_csv( \"/content/drive/MyDrive/THESIS/20220227_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "filenames = list(files_dates[2:4]['files'])\n",
        "combined_csv = pd.concat([pd.read_csv(f, compression='gzip', low_memory=False)for f in filenames ])\n",
        "combined_csv.to_csv( \"/content/drive/MyDrive/THESIS/20220228_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "# split files with multiple dates\n",
        "f = files_dates.at[30,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2022-03-27\").to_csv( \"/content/drive/MyDrive/THESIS/20220327_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2022-03-28\").to_csv( \"/content/drive/MyDrive/THESIS/20220328_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[68,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2022-05-05\").to_csv( \"/content/drive/MyDrive/THESIS/20220505_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2022-05-06\").to_csv( \"/content/drive/MyDrive/THESIS/20220506_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2022-05-07\").to_csv( \"/content/drive/MyDrive/THESIS/20220507_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[98,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2022-06-06\").to_csv( \"/content/drive/MyDrive/THESIS/20220606_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2022-06-07\").to_csv( \"/content/drive/MyDrive/THESIS/20220607_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2022-06-08\").to_csv( \"/content/drive/MyDrive/THESIS/20220608_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[423,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2023-04-29\").to_csv( \"/content/drive/MyDrive/THESIS/20230429_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-04-30\").to_csv( \"/content/drive/MyDrive/THESIS/20230430_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[424,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2023-05-01\").to_csv( \"/content/drive/MyDrive/THESIS/20230501_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-02\").to_csv( \"/content/drive/MyDrive/THESIS/20230502_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[427,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2023-05-05\").to_csv( \"/content/drive/MyDrive/THESIS/20230505_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-06\").to_csv( \"/content/drive/MyDrive/THESIS/20230506_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-07\").to_csv( \"/content/drive/MyDrive/THESIS/20230507_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-08\").to_csv( \"/content/drive/MyDrive/THESIS/20230508_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[430,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2023-05-11\").to_csv( \"/content/drive/MyDrive/THESIS/20230511_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-12\").to_csv( \"/content/drive/MyDrive/THESIS/20230512_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[433,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2023-05-15\").to_csv( \"/content/drive/MyDrive/THESIS/20230515_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-16\").to_csv( \"/content/drive/MyDrive/THESIS/20230516_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "\n",
        "f = files_dates.at[435,'files']\n",
        "twodates = pd.read_csv(f, compression='gzip', engine='python')\n",
        "grouped = twodates.groupby(twodates['tweetcreatedts'].str.split().str[0])\n",
        "grouped.get_group(\"2023-05-18\").to_csv( \"/content/drive/MyDrive/THESIS/20230518_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-19\").to_csv( \"/content/drive/MyDrive/THESIS/20230519_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')\n",
        "grouped.get_group(\"2023-05-20\").to_csv( \"/content/drive/MyDrive/THESIS/20230520_UkraineCombinedTweetsDeduped.csv.gzip\", compression='gzip')"
      ],
      "metadata": {
        "id": "qzbauup25J9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_dates.at[0,'files'] = \"/content/drive/MyDrive/THESIS/20220227_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.at[2,'files'] = \"/content/drive/MyDrive/THESIS/20220228_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[30,'files'] = \"/content/drive/MyDrive/THESIS/20220327_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[30.5] = \"2022-03-28 00:00:00\", \"/content/drive/MyDrive/THESIS/20220328_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[68,'files'] = \"/content/drive/MyDrive/THESIS/20220505_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[68.25] = \"2022-05-06 00:00:00\", \"/content/drive/MyDrive/THESIS/20220506_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[68.5] = \"2022-05-07 00:00:00\", \"/content/drive/MyDrive/THESIS/20220507_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[98,'files'] = \"/content/drive/MyDrive/THESIS/20220606_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[98.25] = \"2022-06-07 00:00:00\", \"/content/drive/MyDrive/THESIS/20220607_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[98.5] = \"2022-06-08 00:00:00\", \"/content/drive/MyDrive/THESIS/20220608_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[423,'files'] = \"/content/drive/MyDrive/THESIS/20230429_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[423.5] = \"2023-04-30 00:00:00\", \"/content/drive/MyDrive/THESIS/20230430_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[424,'files'] = \"/content/drive/MyDrive/THESIS/20230501_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[424.5] = \"2023-05-02 00:00:00\", \"/content/drive/MyDrive/THESIS/20230502_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[427,'files'] = \"/content/drive/MyDrive/THESIS/20230505_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[427.25] = \"2023-05-06 00:00:00\", \"/content/drive/MyDrive/THESIS/20230506_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[427.5] = \"2023-05-07 00:00:00\", \"/content/drive/MyDrive/THESIS/20230507_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[427.75] = \"2023-05-08 00:00:00\", \"/content/drive/MyDrive/THESIS/20230508_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[430,'files'] = \"/content/drive/MyDrive/THESIS/20230511_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[430.5] = \"2023-05-12 00:00:00\", \"/content/drive/MyDrive/THESIS/20230512_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[433,'files'] = \"/content/drive/MyDrive/THESIS/20230515_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[433.5] = \"2023-05-16 00:00:00\", \"/content/drive/MyDrive/THESIS/20230516_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "files_dates.at[435,'files'] = \"/content/drive/MyDrive/THESIS/20230518_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[435.25] = \"2023-05-19 00:00:00\", \"/content/drive/MyDrive/THESIS/20230519_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "files_dates.loc[435.5] = \"2023-05-20 00:00:00\", \"/content/drive/MyDrive/THESIS/20230520_UkraineCombinedTweetsDeduped.csv.gzip\"\n",
        "\n",
        "#sort index\n",
        "files_dates = files_dates.drop([1, 3])\n",
        "files_dates = files_dates.sort_index().reset_index(drop=True)\n",
        "\n",
        "files_dates.to_pickle(\"/content/drive/MyDrive/THESIS/files_final.pkl\")"
      ],
      "metadata": {
        "id": "hQZCQL7wA_YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the tweets for further analysis"
      ],
      "metadata": {
        "id": "IJeFvx4iY5bR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVObcXNfmvp2",
        "outputId": "23a9286b-141e-49bc-9db2-4cfd6315b495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/358.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/358.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.8.0\n"
          ]
        }
      ],
      "source": [
        "pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVrs4zHmi_pY",
        "outputId": "0c087768-c224-4051-856f-ab84a9f56f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from IPython.lib.display import TextDisplayObject\n",
        "import re\n",
        "import html\n",
        "import os\n",
        "import numpy as np\n",
        "import emoji\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import requests\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lmtzr = WordNetLemmatizer()\n",
        "# stopwords, imported from MALLET.\n",
        "stop_words = requests.get('https://raw.githubusercontent.com/mimno/Mallet/master/stoplists/en.txt').text.split()\n",
        "# because of don't in text, people and world are too generally used, the same goes for standwithukraine and video.\n",
        "stop_words = stop_words+['don', 'people', 'world', 'standwithukraine', 'video']\n",
        "files_dates = pd.read_pickle(\"/content/drive/MyDrive/THESIS/files_final.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13y7JarteU_z"
      },
      "outputs": [],
      "source": [
        "# from https://github.com/dr5hn/countries-states-cities-database\n",
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/drive/MyDrive/THESIS/countries+states+cities.json')\n",
        "\n",
        "# dictionary\n",
        "data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c=list()\n",
        "#s=list()\n",
        "for i in range(len(data[232]['states'])):\n",
        "  #s.append(data[232]['states'][i]['name'])\n",
        "  cities = data[232]['states'][i]['cities']\n",
        "  for j in range(len(cities)):\n",
        "    c.append(cities[j]['name'])"
      ],
      "metadata": {
        "id": "CXB9Kan_BYZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# oblasts in Ukraine and Chernobyl because it is not really a city, but important.\n",
        "oblasts = 'Dnipropetrovsk Kharkiv Kyiv Odesa Vinnytsia Donetsk Chernihiv Kamianets-Podilsk Mykolaiv Poltava Zhytomyr Donetsk Kirovohrad Sumy Zaporizhzhia Drohobych Ivano-Frankivsk Lviv Volyn Rivne Tarnopol Chernivtsi Izmail Kherson Zakarpattia Cherkasy Crimea oblast Oblast Chernobyl Chornobyl'.split()"
      ],
      "metadata": {
        "id": "dHbez8H0uzmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#important leaders of ukraine including military generals and currency\n",
        "ukr = ['ukrain', 'zelensk', 'kyiv', 'Reznikov', 'Kuleba', 'Umierov', 'Umerov', 'hryvna', 'hryvnia']"
      ],
      "metadata": {
        "id": "zUglfTmzpS1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#important leaders of russia including military generals, wagner and currency\n",
        "rus = ['russia', 'kremlin', 'putin', 'Shoigu', 'Lavrov', 'peskov', 'wagner', 'Prigozhin', 'ruble', 'rouble']"
      ],
      "metadata": {
        "id": "b0_1DWIdy-NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weapons = ['himars', 'Bayraktar', 'ATACMS', 'Leopard']"
      ],
      "metadata": {
        "id": "tncQpBiFzEIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# other important Ukrainian (former) leaders, russian leaders and wagner leaders.\n",
        "leaders = \"Riabikin Vereshchuk Monastyrsky Gutzeit Lazebna Shkarlet Leshchenko Chernyshov Zaluzhnyi poroshenko turchynov yanukovych Mishustin Utkin Pikalov Gerasimov\".split()"
      ],
      "metadata": {
        "id": "m8iuYtQSrymG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# current ukrainian ministers\n",
        "ministers = [x.split()[1] for x in \"Denys Shmyhal, Yuliia Svyrydenko, Iryna Vereshchuk, Olha Stefanishyna, Mykhailo Fedorov, Oleksandr Kubrakov, Oleh Nemchinov, German Galushchenko, Vadym Huttsait, Oleksandr Kamyshin, Ihor Klymenko, Yulia Laputina, Viktor Liashko, Oksen Lisovyi, Serhii Marchenko, Denys Maliuska, Mykola Solskyi, Ruslan Strilets, Oksana Zholnovych, Rostyslav Karandieiev\".split(',')]"
      ],
      "metadata": {
        "id": "M2vWJhbjrLTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plist = ukr+rus\n",
        "plist = [p.lower() for p in plist]"
      ],
      "metadata": {
        "id": "o3hKPIn7tuqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plist2 = weapons+ministers+leaders\n",
        "plist2 = [p.lower() for p in plist2]"
      ],
      "metadata": {
        "id": "D-8YtFXCftTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plist3 = oblasts+c"
      ],
      "metadata": {
        "id": "DASM85kTHP5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spamlist = '|'.join(['amazon', 'now playing', 'nowplaying'])"
      ],
      "metadata": {
        "id": "rZsFmiiZT1D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing for roberta sentiment as prescribed by source\n",
        "def preprocess(text):\n",
        "    preprocessed_text = []\n",
        "    for t in text.split():\n",
        "        if len(t) > 1:\n",
        "            t = '@user' if t[0] == '@' and t.count('@') == 1 else t\n",
        "            t = 'http' if t.startswith('http') else t\n",
        "        preprocessed_text.append(t)\n",
        "    return ' '.join(preprocessed_text)"
      ],
      "metadata": {
        "id": "kWJwjAhoFpuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uH6jrwYjM2d"
      },
      "outputs": [],
      "source": [
        "#preprocessing\n",
        "for x in tqdm(range(len(files_dates)), position=0, leave=True, desc=\"PREP\", colour='green', ncols=80):\n",
        "  #if os.path.isfile(\"/content/drive/MyDrive/THESIS/clean/%s_cleantext.pkl\" %str(x)):\n",
        "  #  continue\n",
        "  dropw=list()\n",
        "  date = files_dates.at[x,'date']\n",
        "  f = files_dates.at[x,'files']\n",
        "  test = pd.read_csv(f, compression='gzip', engine='python')[['text', 'language', 'location', 'tweetcreatedts', 'followers']]\n",
        "  test = test[test['language'] == 'en'].reset_index(drop=True)\n",
        "  test = test.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
        "\n",
        "  #delete spam that was interfering with the topic results.\n",
        "  test = test[~test.text.str.lower().str.contains(spamlist)].reset_index(drop=True)\n",
        "\n",
        "  for i in tqdm(range(len(test)), position=0, desc=\"TOPIC\", leave=True, colour='red', ncols=80):\n",
        "\n",
        "    text = test.at[i,'text']\n",
        "    # check most common keywords first to speed things up, then the long list of cities.\n",
        "    if any(w in text.lower() for w in plist) == False:\n",
        "      if any(w in text.lower() for w in plist2) == False:\n",
        "        if any(w in text for w in plist3) == False:\n",
        "          dropw.append(i)\n",
        "          continue\n",
        "\n",
        "    #remove new line + tab\n",
        "    text = text.replace(\"\\n\", ' ')\n",
        "    text = text.replace(\"\\t\", ' ')\n",
        "\n",
        "    # replace html with unicode\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # because something with % stopped the loop at remove url\n",
        "    text = re.sub(r'\\S+%\\S+', '', text)\n",
        "\n",
        "    #some random typo I saw in a tweet\n",
        "    if 'Http//' in text:\n",
        "      text = text.replace('Http//', 'http://')\n",
        "\n",
        "    # remove mail\n",
        "    text = re.sub(r\"\\S+@\\S*\\s?\", '', text)\n",
        "\n",
        "    # one instance when url and questionmarks are interfering with remove url.\n",
        "    if 'X?????????????????????????????????????????????????????????????????????????????????????????????????' in text:\n",
        "      text=text.replace('X?????????????????????????????????????????????????????????????????????????????????????????????????', 'X ?????????????????????????????????????????????????????????????????????????????????????????????????')\n",
        "\n",
        "    #only for cardiffnlp/twitter-roberta-base-topic-sentiment-latest\n",
        "    text_r = preprocess(text)\n",
        "\n",
        "    #only for VADER\n",
        "    #remove url\n",
        "    text_v = re.sub(r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', 'http://url_removed', text)\n",
        "    # Replace Twitter Handles like the groundtruth dataset for vader\n",
        "    text_v = re.sub(r'@[\\w]*', '@anonymous', text_v)\n",
        "\n",
        "    # the rest is for the topics\n",
        "    #remove url\n",
        "    text = re.sub(r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', text)\n",
        "\n",
        "    # remove emoji\n",
        "    text = emoji.replace_emoji(text, '')\n",
        "\n",
        "    # remove mentions and hashtags at the end of sentence because these are usually not important for the true subject\n",
        "    text = text.strip()\n",
        "    while re.search('@\\S+$', text.strip()) or re.search('#\\S+$', text.strip()):\n",
        "      text = text.strip()\n",
        "      if re.search('[\\.|\\!|\\?]$', text):\n",
        "        break\n",
        "      s = re.findall('#\\S+$', text.strip())\n",
        "      a = re.findall('@\\S+$', text.strip())\n",
        "      if s:\n",
        "        text = text.replace(s[0], '')\n",
        "      if a:\n",
        "        text = text.replace(a[0], '')\n",
        "\n",
        "    #remove via @...\n",
        "    pattern = \"\\s+via\\s+@\\S+\"\n",
        "    match = re.findall(pattern, text)\n",
        "    for m in match:\n",
        "      text = text.replace(m, '')\n",
        "\n",
        "    #delete string of multiple #\n",
        "    while re.search('((#\\S+(\\s+|$)){2,})', text):\n",
        "      r = re.search('((#\\S+(\\s+|$)){2,})', text)\n",
        "      mentions = r.group(1)\n",
        "      text = text.replace(mentions, '')\n",
        "\n",
        "    #delete string of multiple @\n",
        "    while re.search('((@\\S+(\\s+|$)){2,})', text):\n",
        "      r = re.search('((@\\S+(\\s+|$)){2,})', text)\n",
        "      mentions = r.group(1)\n",
        "      text = text.replace(mentions, '')\n",
        "\n",
        "    # delete @ at start of sentence\n",
        "    text = text.strip()\n",
        "    while re.search('\\A@\\S+\\s', text.strip()):\n",
        "      text = text.strip()\n",
        "      r = re.findall('\\A@\\S+\\s', text)\n",
        "      if r:\n",
        "        text = text.replace(r[0], '')\n",
        "\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z ]\", \" \", text)\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'\\s+', \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "    text = [w for w in text if not len(w) <= 2 or w =='un' or w == 'eu' or w == 'uk' or w == 'ua']\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    text = [lmtzr.lemmatize(w) for w in text]\n",
        "\n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # because lemmatize replaces states with state\n",
        "    for m in re.findall(\"united state\\s\", text):\n",
        "      text = text.replace(m, 'united states ')\n",
        "    for m in re.findall(\"united state$\", text):\n",
        "      text = text.replace(m, 'united states')\n",
        "\n",
        "    test.at[i, 'cleantext'] = text\n",
        "    test.at[i, 'vadertext'] = text_v\n",
        "    test.at[i, 'robertatext'] = text_r\n",
        "\n",
        "  test= test.drop(dropw, axis='index').reset_index(drop=True)\n",
        "  test=test.drop_duplicates(subset=['cleantext']).reset_index(drop=True)\n",
        "  doc_list = [l.split() for l in list(test['cleantext'])]\n",
        "  test['tokens'] = doc_list\n",
        "  test = test[test.tokens.map(len)>3].reset_index(drop=True)\n",
        "\n",
        "  test.to_pickle(\"/content/drive/MyDrive/THESIS/clean/%s_cleantext.pkl\" %str(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run VADER, CIDER and NMF"
      ],
      "metadata": {
        "id": "I2FJ9p59Z_5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ciderpolarity"
      ],
      "metadata": {
        "id": "iO-BpZJtZ_Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ciderpolarity import CIDER\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn import decomposition\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "from ast import literal_eval\n",
        "from statistics import multimode\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import random\n",
        "from google.colab.data_table import DataTable\n",
        "DataTable.max_columns = 61\n",
        "from collections import Counter\n",
        "\n",
        "# https://github.com/dlukes/rbo\n",
        "from drive.MyDrive.rbo import rbo\n",
        "# load file with info on dates and files\n",
        "files_dates = pd.read_pickle(\"/content/drive/MyDrive/THESIS/ultimate_files.pkl\")\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "9dKsXLpKaPtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set seeds\n",
        "random.seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "id": "5wuP61B9adKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iawILq-38q2i"
      },
      "outputs": [],
      "source": [
        "# the amount of topics is fixed with k=10\n",
        "k=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiOzqwG8d_1J"
      },
      "outputs": [],
      "source": [
        "# base dataframe\n",
        "dfl=dict()\n",
        "dfl['date'] = 'NaN'\n",
        "for i in range(k):\n",
        "  dfl['topic_rank%s'%str(i)] = [i]\n",
        "  dfl['rbo_score%s'%str(i)] = 'NaN'\n",
        "  dfl['mean_vader%s'%str(i)] = 'NaN'\n",
        "  dfl['mean_cider_sentiment%s'%str(i)] = 'NaN'\n",
        "  dfl['mean_cider_intensity%s'%str(i)] = 'NaN'\n",
        "  dfl['topic_words%s'%str(i)] = 'NaN'\n",
        "df = pd.DataFrame(dfl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIDER sentiment and intensity to individual data files per day\n",
        "for x in tqdm(range(len(files_dates)), position=0, leave=True, desc=\"CIDER\", colour='green', ncols=80):\n",
        "  test = pd.read_pickle(\"/content/drive/MyDrive/THESIS/clean/%s_cleantext.pkl\" %str(x))\n",
        "  input_data = list(test['vadertext'])\n",
        "  cdr = CIDER(input_data, output_folder, no_below=100)\n",
        "  cdr=cdr.fit_transform()\n",
        "  test['cider_sentiment'] = [c[1]['compound'] for c in cdr]\n",
        "  test['cider_intensity'] = [c[1]['intensity'] for c in cdr]\n",
        "  test.to_pickle(\"/content/drive/MyDrive/THESIS/clean/%s_cleantext.pkl\" %str(x))"
      ],
      "metadata": {
        "id": "HhCcLRS7sO9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in tqdm(range(len(files_dates)), position=0, leave=True, desc=\"TOPIC\", colour='green', ncols=80):\n",
        "\n",
        "  # open datafile\n",
        "  test = pd.read_pickle(\"/content/drive/MyDrive/THESIS/clean/%s_cleantext.pkl\" %str(x))\n",
        "\n",
        "  # the following section of NMF topic modeling uses a modified version of the code found on the following website: https://github.com/nuitrcs/topic-modeling-workshop/tree/master\n",
        "  tfidf_vectorizer = TfidfVectorizer(min_df=30)\n",
        "  tfidf = tfidf_vectorizer.fit_transform(test['cleantext'])\n",
        "\n",
        "  # Apply\n",
        "  model = decomposition.NMF(solver='mu', init = 'nndsvda', n_components=k )\n",
        "  W = model.fit_transform( tfidf )\n",
        "  H = model.components_\n",
        "  terms = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "  #choose topic for tweet based on probablities\n",
        "  #show topic and corresponding probability\n",
        "  for t in range(len(W)):\n",
        "    topic = W[t].argmax()\n",
        "    prob = W[t][W[t].argmax()]\n",
        "    test.at[t,'topic'] = topic\n",
        "    test.at[t,'topic_prob'] = prob\n",
        "\n",
        "  # rearrange the topic position and name based on popularity\n",
        "  old_order = [i for i in test.value_counts(subset='topic').index.tolist()]\n",
        "  new_order = [float(i) for i in range(len(old_order))]\n",
        "  change_topic = {'topic' : dict(zip(old_order, new_order))}\n",
        "  test = test.replace(change_topic)\n",
        "\n",
        "  # for every tweet calculate sentiment and save in data file\n",
        "  for i in tqdm(range(len(test)), position=0, desc=\"VADER\", leave=True, colour='red', ncols=80):\n",
        "    vader = test.at[i,'vadertext']\n",
        "    v = sid.polarity_scores(vader)\n",
        "    test.at[i,'vader_sentiment']=str(v)\n",
        "\n",
        "\n",
        "  # save file\n",
        "  test.to_pickle(\"/content/drive/MyDrive/THESIS/clean/%s_cleantext.pkl\" %str(x))\n",
        "\n",
        "  termsl=[]\n",
        "  weightsl=[]\n",
        "  for topic_index in range(k):\n",
        "    # get the top terms and their weights\n",
        "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
        "    top_terms = []\n",
        "    top_weights = []\n",
        "    for term_index in top_indices[0:10]:\n",
        "      top_terms.append( terms[term_index] )\n",
        "      top_weights.append( H[topic_index,term_index] )\n",
        "    termsl.append(top_terms)\n",
        "    weightsl.append(top_weights)\n",
        "\n",
        "  # like before, change the topic order based on rankings so it maches the datafile\n",
        "  order = [int(i) for i in test.value_counts(subset='topic').index.tolist()]\n",
        "  termsl = np.asarray(termsl)[order].tolist()\n",
        "  weightsl = np.asarray(weightsl)[order].tolist()\n",
        "  files_dates.at[x,'topics']=str(termsl)\n",
        "  files_dates.at[x,'weights']=str(weightsl)\n",
        "\n",
        "\n",
        "  coher = []\n",
        "  if x != 0:\n",
        "    for i in range(len(prevtop)):\n",
        "      pr = prevtop[i]\n",
        "      for j in range(len(termsl)):\n",
        "        c = termsl[j]\n",
        "        coher = coher + [[i,j,rbo(pr,c,p=0.737)[2]]]\n",
        "\n",
        "\n",
        "    l1 = list()\n",
        "    l2 = list()\n",
        "    coher = sorted(coher, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    for i in coher:\n",
        "      if i[0] not in l1:\n",
        "        if i[1] not in l2:\n",
        "          l1.append(i[0])\n",
        "          l2.append(i[1])\n",
        "          p = list(max([df.loc[x-1,['topic_rank'+str(t) for t in range(k)]].eq(i[0])])).index(True)\n",
        "          df.at[x,'topic_rank'+str(p)]=i[1]\n",
        "          df.at[x,'rbo_score'+str(p)]=i[2]\n",
        "          df.at[x,'topic_words'+str(p)]=termsl[i[1]]\n",
        "\n",
        "  else:\n",
        "    for i in range(len(termsl)):\n",
        "      df.at[x,'topic_words'+str(i)]=termsl[i]\n",
        "\n",
        "  prevtop = termsl\n",
        "  #add date to df\n",
        "  df.at[x, 'date']=dates[x]\n",
        "  # find mean VADER and RoBERTa sentiment for every topic and mode RoBERTa\n",
        "  for i in range(k):\n",
        "    index = test.index[test['topic'] == i].tolist()\n",
        "    for j in range(len(index)):\n",
        "      if j == 0:\n",
        "        va = np.array(list(literal_eval(test.loc[index[j],'vader_sentiment']).values()))\n",
        "        ci_s = test.loc[index[j],'cider_sentiment']\n",
        "        ci_i = test.loc[index[j],'cider_intensity']\n",
        "      else:\n",
        "        va = va + np.array(list(literal_eval(test.loc[index[j],'vader_sentiment']).values()))\n",
        "        ci_s += test.loc[index[j],'cider_sentiment']\n",
        "        ci_i += test.loc[index[j],'cider_intensity']\n",
        "    mean_vader = va/len(index)\n",
        "    mean_cider_sentiment = ci_s/len(index)\n",
        "    mean_cider_intensity = ci_i/len(index)\n",
        "\n",
        "    p = list(max([df.loc[x,['topic_rank'+str(t) for t in range(k)]].eq(i)])).index(True)\n",
        "    df.at[x,'mean_vader'+str(p)] = mean_vader[3]\n",
        "    df.at[x,'mean_cider_sentiment'+str(p)] = mean_cider_sentiment\n",
        "    df.at[x,'mean_cider_intensity'+str(p)] = mean_cider_intensity\n",
        "\n",
        "\n",
        "  files_dates.to_pickle(\"/content/drive/MyDrive/THESIS/ultimate_files.pkl\")\n",
        "  df.to_pickle(\"/content/drive/MyDrive/THESIS/final_data.pkl\")"
      ],
      "metadata": {
        "id": "qIZGO_xsbJIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect datafile\n",
        "df"
      ],
      "metadata": {
        "id": "M30WkZtscDAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next are some functions to help with analysing the data\n",
        "\n",
        "# function to transform lists of sentiment compound scores to labels\n",
        "def sentiment(list_of_sentiment_scores):\n",
        "  labels=list()\n",
        "  for x in list_of_sentiment_scores:\n",
        "    if x >= 0.05:\n",
        "      labels.append('positive')\n",
        "    if x <= -0.05:\n",
        "      labels.append('negative')\n",
        "    if x > -0.05 and x < 0.05:\n",
        "      labels.append('neutral')\n",
        "  return labels\n",
        "\n",
        "# function for mean sentiment or intensity\n",
        "def mean_list(list_of_values):\n",
        "  return sum(list_of_values)/len(list_of_values)\n",
        "\n",
        "# percentage of sentiment cathegories\n",
        "def percentage_sentiment(list_of_sentiment_labels):\n",
        "  return dict(zip(list(Counter(list_of_sentiment_labels).keys()),\n",
        "   [v/(len(list_of_sentiment_labels))*100 for v in\n",
        "    list(Counter(list_of_sentiment_labels).values())]))\n",
        "\n",
        "# Make a weighted average of the topics.\n",
        "# The first word has 10 times more weight than the last.\n",
        "def averaged_topic(list_of_topics):\n",
        "  d=dict()\n",
        "  for topic in list_of_topics:\n",
        "    c=10\n",
        "    for word in topic:\n",
        "      if word in d:\n",
        "        d[word]=d[word]+c\n",
        "        c-=1\n",
        "      else:\n",
        "        d[word]=c\n",
        "        c-=1\n",
        "  return list(d.keys())[0:10]\n",
        "\n",
        "# a def to make combining topics easier.\n",
        "def topic_track(list_of_index_and_position_topic):\n",
        "  topic_rank=list()\n",
        "  topic_words=list()\n",
        "  vader=list()\n",
        "  cider_sentiment=list()\n",
        "  cider_intensity=list()\n",
        "  for i in list_of_index_and_position_topic:\n",
        "    if len(i[1])==2:\n",
        "      start=i[1][0]\n",
        "      end=i[1][1]\n",
        "      topic_rank = topic_rank + list(df['topic_rank%s' %str(i[0])][start:end])\n",
        "      topic_words = topic_words + list(df['topic_words%s' %str(i[0])][start:end])\n",
        "      vader = vader + list(df['mean_vader%s'%str(i[0])][start:end])\n",
        "      cider_sentiment = cider_sentiment + list(df['mean_cider_sentiment%s'%str(i[0])][start:end])\n",
        "      cider_intensity = cider_intensity + list(df['mean_cider_intensity%s'%str(i[0])][start:end])\n",
        "    else:\n",
        "      topic_rank = topic_rank + [df['topic_rank%s' %str(i[0])][i[1][0]]]\n",
        "      topic_words = topic_words + [df['topic_words%s'%str(i[0])][i[1][0]]]\n",
        "      vader = vader + [df['mean_vader%s'%str(i[0])][i[1][0]]]\n",
        "      cider_sentiment = cider_sentiment + [df['mean_cider_sentiment%s'%str(i[0])][i[1][0]]]\n",
        "      cider_intensity = cider_intensity + [df['mean_cider_intensity%s'%str(i[0])][i[1][0]]]\n",
        "  return topic_rank, topic_words, vader, cider_sentiment, cider_intensity"
      ],
      "metadata": {
        "id": "a0XH1CGabuKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiments for ukraine topic (not using the topic_track function)\n",
        "ukraine_topic_rank= list(df['topic_rank0'][0:326])+list(df['topic_rank8'][327:399])+list(df['topic_rank2'][401:472])\n",
        "ukraine_topic_words = list(df['topic_words0'][0:326])+list(df['topic_words8'][327:399])+list(df['topic_words2'][401:472])\n",
        "ukraine_vader = list(df['mean_vader0'][0:326])+list(df['mean_vader8'][327:399])+list(df['mean_vader2'][401:472])\n",
        "mean_ukraine_vader = mean_list(ukraine_vader)\n",
        "ukraine_cider_sentiment = list(df['mean_cider_sentiment0'][0:326])+list(df['mean_cider_sentiment8'][327:399])+list(df['mean_cider_sentiment2'][401:472])\n",
        "mean_ukraine_cider_sentiment = mean_list(ukraine_cider_sentiment)\n",
        "ukraine_cider_intensity = list(df['mean_cider_intensity0'][0:326])+list(df['mean_cider_intensity8'][327:399])+list(df['mean_cider_intensity2'][401:472])\n",
        "mean_ukraine_cider_intensity = mean_list(ukraine_cider_intensity)"
      ],
      "metadata": {
        "id": "Db92lT4XokYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_topic(ukraine_topic_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-kof1LGFpH6",
        "outputId": "d893aef5-e8bd-4dea-b32f-fa5bca37f0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ukraine',\n",
              " 'support',\n",
              " 'stand',\n",
              " 'invasion',\n",
              " 'indian',\n",
              " 'today',\n",
              " 'crisis',\n",
              " 'student',\n",
              " 'conflict',\n",
              " 'attack']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('mean_ukraine_vader = ' + str(mean_ukraine_vader))\n",
        "print('mean_ukraine_cider_sentiment = ' + str(mean_ukraine_cider_sentiment))\n",
        "print('mean_ukraine_cider_intensity = ' + str(mean_ukraine_cider_intensity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HevNoElHd8T5",
        "outputId": "97216c7c-8036-4387-8890-1f5bb91973df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_ukraine_vader = -0.01956012495224538\n",
            "mean_ukraine_cider_sentiment = 0.008791233077198455\n",
            "mean_ukraine_cider_intensity = 0.5496775227760493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate percentages (positive, negative, neutral)\n",
        "print(percentage_sentiment(sentiment(ukraine_vader)))\n",
        "print(percentage_sentiment(sentiment(ukraine_cider_sentiment)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao-HaSs4YGHB",
        "outputId": "298cf079-6e88-4834-a266-c51ede8b2862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'positive': 47.12153518123667, 'negative': 39.23240938166311, 'neutral': 13.646055437100213}\n",
            "{'positive': 48.18763326226013, 'neutral': 15.778251599147122, 'negative': 36.034115138592746}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(ukraine_topic_rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7YMpJeosPzC",
        "outputId": "6764815a-f0e9-44ee-b685-3a0cb97db4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0.0: 302, 4.0: 29, 6.0: 4, 5.0: 6, 2.0: 23, 1.0: 87, 3.0: 17, 7.0: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(['support' in t for t in ukraine_topic_words]))\n",
        "print(Counter([t[1]=='support' for t in ukraine_topic_words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbTjBwRBdDwi",
        "outputId": "22bee7da-4616-47b2-ded5-7e65bf0fef74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({True: 393, False: 76})\n",
            "Counter({True: 339, False: 130})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiments for russian army topic (not using the topic_track function)\n",
        "russian_topic_rank= list(df['topic_rank1'][0:423]) + [df['topic_rank6'][425]] + list(df['topic_rank3'][427:472])\n",
        "\n",
        "russian_topic_words= list(df['topic_words1'][0:423])+[df['topic_words6'][425]]+list(df['topic_words3'][427:472])\n",
        "russian_vader = list(df['mean_vader1'][0:423])+[df['mean_vader6'][425]]+list(df['mean_vader3'][427:472])\n",
        "mean_russian_vader = mean_list(russian_vader)\n",
        "\n",
        "russian_cider_sentiment = list(df['mean_cider_sentiment1'][0:423])+[df['mean_cider_sentiment6'][425]]+list(df['mean_cider_sentiment3'][427:472])\n",
        "mean_russian_cider_sentiment = mean_list(russian_cider_sentiment)\n",
        "\n",
        "russian_cider_intensity = list(df['mean_cider_intensity1'][0:423])+[df['mean_cider_intensity6'][425]]+list(df['mean_cider_intensity3'][427:472])\n",
        "mean_russian_cider_intensity = mean_list(russian_cider_intensity)"
      ],
      "metadata": {
        "id": "7pLneuHVm2xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_topic(russian_topic_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDZTCyGnGPA5",
        "outputId": "0cff5fa7-1269-49fd-dfc7-da1becf64ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['russian',\n",
              " 'force',\n",
              " 'military',\n",
              " 'soldier',\n",
              " 'tank',\n",
              " 'troop',\n",
              " 'destroyed',\n",
              " 'army',\n",
              " 'kharkiv',\n",
              " 'invasion']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('mean_russian_vader = ' + str(mean_russian_vader ))\n",
        "print('mean_russian_cider_sentiment = ' + str(mean_russian_cider_sentiment))\n",
        "print('mean_russian_cider_intensity = ' + str(mean_russian_cider_intensity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5FAFHjWoY0_",
        "outputId": "1c4959dd-4807-47a2-85a7-5f32e1a619d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_russian_vader = -0.08700813615126454\n",
            "mean_russian_cider_sentiment = -0.06066397745465472\n",
            "mean_russian_cider_intensity = 0.5453239007758945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate percentages (positive, negative, neutral)\n",
        "print(percentage_sentiment(sentiment(russian_vader)))\n",
        "print(percentage_sentiment(sentiment(russian_cider_sentiment)))"
      ],
      "metadata": {
        "id": "fsyinwhe2LMt",
        "outputId": "13ff7f16-c62f-457e-d529-c9222d34c83a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'negative': 66.95095948827291, 'positive': 23.02771855010661, 'neutral': 10.021321961620469}\n",
            "{'positive': 22.81449893390192, 'negative': 59.27505330490405, 'neutral': 17.91044776119403}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(russian_topic_rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWkGjJ0nspdH",
        "outputId": "d92300f4-c5d1-4483-807d-010383f38389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1.0: 256,\n",
              "         3.0: 25,\n",
              "         2.0: 60,\n",
              "         0.0: 69,\n",
              "         6.0: 16,\n",
              "         7.0: 7,\n",
              "         5.0: 9,\n",
              "         4.0: 18,\n",
              "         8.0: 7,\n",
              "         9.0: 2})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(russian_topic_rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXIVsH83tB4m",
        "outputId": "6ec5ee02-57fb-408f-de02-eef9895b1866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "469"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate percentages (positive, negative, neutral)\n",
        "print(percentage_sentiment(sentiment(russian_vader)))\n",
        "print(percentage_sentiment(sentiment(russian_cider_sentiment)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXN3Gj9_r_J_",
        "outputId": "4bf314c9-8912-4dc3-834a-f317dfde69a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'negative': 66.95095948827291, 'positive': 23.02771855010661, 'neutral': 10.021321961620469}\n",
            "{'positive': 22.81449893390192, 'negative': 59.27505330490405, 'neutral': 17.91044776119403}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#topic on war\n",
        "war_dict = [[2,[0,69]], [6,[71,181]], [9,[183,194]], [8,[196,290]], [5,[292,297]], [9,[299,377]], [2,[380,398]], [8,[400,411]], [0,[413,422]], [4,[424,472]]]\n",
        "\n",
        "war_topic_rank, war_topic_words, war_vader, war_cider_sentiment, war_cider_intensity = topic_track(war_dict)\n",
        "mean_war_vader = mean_list(war_vader)\n",
        "mean_war_cider_sentiment = mean_list(war_cider_sentiment)\n",
        "mean_war_cider_intensity = mean_list(war_cider_intensity)"
      ],
      "metadata": {
        "id": "Cef5pHw2JrDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(averaged_topic(war_topic_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRtlX2roRgvQ",
        "outputId": "5a5ea693-9903-4d14-bc5c-67d5dbae2d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['war', 'stop', 'peace', 'end', 'protest', 'crime', 'anti', 'humanity', 'innocent', 'life']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_war_vader)\n",
        "print(mean_war_cider_sentiment)\n",
        "print(mean_war_cider_intensity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmf0mSOxSJcr",
        "outputId": "c1a48021-f580-4a6f-de4d-9d05f217e43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.19624784051653055\n",
            "-0.13438414577450075\n",
            "0.55839638705083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(percentage_sentiment(sentiment(war_vader)))\n",
        "print(percentage_sentiment(sentiment(war_cider_sentiment)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk9HATYgTJJU",
        "outputId": "44ebd73e-89a0-43c0-eeec-4eca90391629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'negative': 84.32671081677704, 'neutral': 10.596026490066226, 'positive': 5.077262693156733}\n",
            "{'positive': 7.28476821192053, 'neutral': 19.867549668874172, 'negative': 72.84768211920529}\n"
          ]
        }
      ]
    }
  ]
}